{"text": " Every single machine learning algorithm explained. In case you don't know an algorithm is a set of commands that must be followed for a computer to perform calculations or like other problem-solving operations. According to its formal definition, an algorithm is a finite set of instructions carried out in a specific order to perform a particular task. It's not an entire program or code, it is simple logic to a problem. Linear regression is a supervised learning algorithm and tries to model their relationship between a continuous target variable and one or more independent variables by fitting a linear equation to the data. Take this chart of dots, for example. A linear regression model tries to fit a regression line to the data points that best represents their relations or correlations. With this method, the best regression line is found by minimizing the sum of squares of the distance between the data points and the regression line. So for these data points, their regression line looks like this. Support vector machine or SVM for short is a supervised learning algorithm and is mostly used for classification tasks but is also suitable for regression tasks. SVM distinguishes classes by drawing a decision boundary. How to draw or determine the decision boundary is the most critical part in SVM algorithms. Before creating the decision boundary, each observation or data point is plotted in n-dimensional space with n being the number of features used. For example, if we use a length and width to classify different cells, observations are plotted in a two-dimensional space and decision boundary is aligned. If we use three features, the decision boundary is a plane in three-dimensional space. If we use more than three features, the decision boundary becomes a hyperplane which is really hard to visualize. The decision boundary is drawn in a way that the distance to support vectors are maximized. If the decision boundary is too close to a support vector, it will be highly sensitive to noises and not generalize well. Even very small changes to independent variables may cause a misclassification. SVM is a specially effective in cases where number of dimensions are more than the number of samples. When finding the decision boundary, SVM uses a subset of training points rather than all points, which makes it memory efficient. On the other hand, training time increases for large data sets, which negatively affects the performance. Naive Bayes is a supervised learning algorithm used for classification tasks. Hence, it is also called Naive Bayes classifier. Naive Bayes assumes that features are independent of each other and there is no correlation between features. However, this is not the case in real life. This naive assumption of features being uncorrelated is the reason why this algorithm is called naive. The intuition behind naive Bayes algorithm is the Bayes theorem. PAB is the probability of events A given events B has already occurred. PBA is the probability of events B given events A has already occurred. PA is the probability of events A and PB is the probability of events B. Naive Bayes classifier calculates the probability of a class given a set of feature values. The assumption that all features are independent makes Naive Bayes algorithm very fast when compared to complicated algorithms. In some cases, speed is preferred over higher accuracy. But on the other hand, the same assumption makes Naive Bayes algorithm less accurate than complicated algorithms. Logistic regression is a supervised learning algorithm which is mostly used for binary classification problems. Logistic regression is a simple yet very effective classification algorithm, so it is commonly used for many binary classification tasks. Things like customer churns, spam, email, website, or ad click predictions are some examples of the areas where logistic regression offers a powerful solution. The basis of logistic regression is the logistic function, also called the sigmoid function, which takes any real value number and maps it to a value between zero and one. Let's consider we have the following linear equation to solve. Logistic regression model takes a linear equation as input and uses logistic function and log to perform a binary classification task. Then we will get the famous shaped graph of logistic regression. We can use the calculated probability as is. For example, the output can be the probability that this email is spam is 95%, or the probability that the customer will click on the ad is 70%. However, in most cases, probabilities are used to classify data points. For example, if the probability is greater than 50%, the prediction is positive class, or one. Otherwise, the prediction is negative class or zero. K nearest neighbors, or K and N for short, is a supervised learning algorithm that can be used to solve both classification and regression tasks. The main idea behind K and N is that the value of a class or of a data point is determined by the data points around it. K and N class fire determines the class of a data point by majority voting principle. For instance, if K is set to five, the classes of five closest points are checked. Prediction is done according to the majority class. Similarly, K and N regression takes the mean value of five closest points. Let's go over an example. Consider the following data points that belong to four different classes. And let's see how the predicted classes change according to the K value. It is very important to determine an optimal K value. If K is too low, the model is too specific and not generalized well. It also tends to be too sensitive to noise. The model accomplishes a high accuracy on train set, but will be a poor predictor on new previously unseen data points. Therefore, we are likely to end up with an overfit model. On the other hand, if K is too large, the model is too generalized and does not have a predictor on both train and test sets. The situation is known as underfitting. K and N is simple and easy to interpret. It does not make any assumptions, so it can be implemented in non-linear tasks. K and N does become very slow as number of data points increases because the model needs to store all data points. Thus, it is not memory efficient. Another downside of K and N is that it is sensitive to outliers. Decision trees work by iteratively asking questions to partition data. It is easier to conceptualize the partitioning data with a visual representation of the decision tree. This represents a decision tree to predict customer churn. First, split is based on monthly chart's amount. Then the algorithm keeps asking questions to separate class labels. The question gets more specific as the tree gets deeper. The aim is to increase the predictivis as much as possible at each partitioning so that the model keeps gaining information about the data set. Randomly splitting, the feature does not usually give us the valuable insight into the data set. It lets that increase purity of nodes that are most informative. The purity of a node is inversely proportional to the distribution of different classes in that node. The questions to ask are chosen in a way that increases purity or decreases impurity. But how many questions do we ask when do we stop? When is our tree sufficient to solve our classification problem? The answer to all of these questions leads us to one of the most important concepts in machine learning overfitting. The model can keep asking questions until all nodes are pure. However, this would be a two specific model and it would not generalize will. It achieves high accuracy with training set but performs poorly on new previously unseen data points which indicates overfitting. The decision tree algorithm usually does not require to normalize or scale features. It is also suitable to work on a mixture of feature data types. On the negative side, it is prone to overfitting and needs to be unsombled in order to generalize will. Random forest is an ensemble of many decision trees. Random forests are built using a method called bagging in which decision trees are used as parallel estimators. If you use for a classification problem, the result is based on majority vote of the results received from each decision tree. For regression, the prediction of a leaf node is the mean value of the target values in that leaf. Random forest regression takes mean values of results from decision trees. Random forests reduce the risk of overfitting and accuracy is much higher than a single decision tree. Furthermore, decision trees in a random forest run in parallel so that the time does not become a bottleneck. The success of a random forest highly depends on using uncorrelated decision trees. If we use the same or very similar trees, the overall result will not be much different than the result of a single decision tree. Random forests achieve to have uncorrelated decision trees by bootstrapping and feature randomness. Bootstrapping is randomly selecting samples from training data with replacement. They are called the bootstrapped samples. Feature randomness is achieved by selecting features randomly for each decision tree in a random forest. The number of features used for each tree in a random forest can be controlled with max underscore features parameter. Random forest is a highly accurate model on many different problems and does not require normalization or scaling. However, it is not a good choice for high-dimensional datasets compared to fast linear models. Gradient boosted decision trees or GBDT for short is an ensemble algorithm which uses boosting methods to combine individual decision trees. Boosting means combining a learning algorithm in series to achieve a strong learner from many sequentially connected weak learners. In the case of GBDT, the weak learners are the decision trees. Each tree attempts to minimize the errors of previous tree. Trees in boosting are weak learners but adding many trees in series and each focusing on the errors from the previous one make boosting a highly efficient and accurate model. Unlike bagging, boosted does not involve bootstrapped sampling. Every time a new tree is added, it fits on a modified version of the initial dataset. Since trees are added sequentially, boosting algorithms learn slowly. In statistical learning, models that learn slowly perform better. GBDT is very efficient on both classification and regression tasks and provides more accurate predictions compared to random forests. It can handle mixed type of features and no preprocessing is needed. GBDT does require careful tuning of hyper parameters in order to prevent the model from overfitting. K means clustering. Clustering is a way to group a set of data points in a way that similar data points are grouped together. Therefore, clustering algorithms look for similarities or dissimilarities among data points. Clustering is an unsupervised learning method so there is no label associated with data points. Clustering algorithms try to find the underlying structure of the data. Observations or data points in a classification task have labelled. Each observation is classified according to some measurements. Classification algorithms try to model the relationship between measurements on observations and their assigned class. Then the model predicts the class of new observations. K means clustering aims to partition data into K clusters in a way that data points in the same cluster are similar and data points in different clusters are further apart. Thus, it is a partition-based clustering technique. Similarity of two points is determined by the distance between them. Consider the following to the visualization of a dataset. It can be parsioned into four different clusters. Now, real-life datasets are much more complex in which clusters are not clearly separated. However, the algorithm works in the same way. K means is an iterative process. It is built on expectation, maximization, algorithm. After the number of clusters are determined, it works by executing the following steps. Number one, it randomly selects the centroid or the center of cluster for each cluster. Then it calculates the distance of all data points to the centroid. It assigns the data points to the closest cluster. It finds the new centroid of each cluster by taking the mean of all data points in the cluster. It repeats steps to three and four until all points converge and cluster centers stop moving. K means clustering is relatively fast and easy to interpret. It is also able to choose the positions of initial centroid in a smart way that speeds up the convergence. The one challenge with K means is that the number of clusters must be predetermined. K means algorithm is not able to guess how many clusters exist in the data. If there is a non-linear structure separating groups in the data, K means will not be a good choice. DB Stan clustering Partition based and hierarchical clustering techniques are highly efficient with normal shaped clusters. However, when it comes to arbitrary shaped clusters or detecting outliers, density based techniques are more efficient. DB Stan stands for density based spatial clustering of applications with noise. It is able to find arbitrary shaped clusters and clusters with noise. The main idea behind DB Stan is that a point belongs to a cluster if it is close to many points from that cluster. There are two key parameters of DB Stan. EPS, which is the distance that specifies the neighborhood. Two points are considered to be neighbors if the distance between them are less than or equal to EPS and menPTS, which is the minimum number of data points to define a cluster. Based on these two parameters, points are classified as core point, border point, or outlet. A point is a core point if there are at least menPTS, number of points, including the point itself, in its surrounding area with radius EPS. A point is a border point if it is unreachable from a core point and there are less than menPTS number of points within its surrounding area. And a point is an outlier if it is not a core point and not reachable from any core points. DB Stan does not require to specify a number of clusters beforehand. It is robust to outliers and able to detect the outliers. In some cases, determining an appropriate distance of neighborhood EPS is not easy and it requires domain knowledge. Principal components analysis or PCA is a dimensionally reduction algorithm which basically derives new features from the existing ones with keeping as much information as possible. PCA is an unsupervised learning algorithm, but it is also widely used as a pre-processing step for supervised learning algorithms. PCA derives new features by finding the relations among features in a dataset. The aim of PCA is to explain the variance within the original dataset as much as possible by using less features. The new derived features are called principal components. The order of principal components is determined according to the fraction of variants of original dataset they explain. The advantage of PCA is that a significant amount of variance of the original dataset is retained using much smaller number of features than the original dataset. Principal components are ordered according to the amount of variants that they explain, and that is every common machine learning algorithm explained.", "segments": [{"start": 0.0, "end": 4.64, "text": " Every single machine learning algorithm explained. In case you don't know an algorithm is a set of"}, {"start": 4.64, "end": 9.6, "text": " commands that must be followed for a computer to perform calculations or like other problem-solving"}, {"start": 9.6, "end": 14.4, "text": " operations. According to its formal definition, an algorithm is a finite set of instructions"}, {"start": 14.4, "end": 19.92, "text": " carried out in a specific order to perform a particular task. It's not an entire program or code,"}, {"start": 19.92, "end": 25.36, "text": " it is simple logic to a problem. Linear regression is a supervised learning algorithm and tries to"}, {"start": 25.36, "end": 30.56, "text": " model their relationship between a continuous target variable and one or more independent variables"}, {"start": 30.56, "end": 35.68, "text": " by fitting a linear equation to the data. Take this chart of dots, for example. A linear regression"}, {"start": 35.68, "end": 40.0, "text": " model tries to fit a regression line to the data points that best represents their relations or"}, {"start": 40.0, "end": 45.44, "text": " correlations. With this method, the best regression line is found by minimizing the sum of squares"}, {"start": 45.44, "end": 49.519999999999996, "text": " of the distance between the data points and the regression line. So for these data points,"}, {"start": 49.52, "end": 55.120000000000005, "text": " their regression line looks like this. Support vector machine or SVM for short is a supervised"}, {"start": 55.120000000000005, "end": 60.800000000000004, "text": " learning algorithm and is mostly used for classification tasks but is also suitable for regression tasks."}, {"start": 60.800000000000004, "end": 66.0, "text": " SVM distinguishes classes by drawing a decision boundary. How to draw or determine the"}, {"start": 66.0, "end": 71.52000000000001, "text": " decision boundary is the most critical part in SVM algorithms. Before creating the decision boundary,"}, {"start": 71.52000000000001, "end": 77.92, "text": " each observation or data point is plotted in n-dimensional space with n being the number of features"}, {"start": 78.4, "end": 83.76, "text": " used. For example, if we use a length and width to classify different cells, observations are"}, {"start": 83.76, "end": 89.28, "text": " plotted in a two-dimensional space and decision boundary is aligned. If we use three features,"}, {"start": 89.28, "end": 94.56, "text": " the decision boundary is a plane in three-dimensional space. If we use more than three features,"}, {"start": 94.56, "end": 99.68, "text": " the decision boundary becomes a hyperplane which is really hard to visualize. The decision boundary"}, {"start": 99.68, "end": 104.96000000000001, "text": " is drawn in a way that the distance to support vectors are maximized. If the decision boundary is too"}, {"start": 104.96, "end": 110.24, "text": " close to a support vector, it will be highly sensitive to noises and not generalize well."}, {"start": 110.24, "end": 115.75999999999999, "text": " Even very small changes to independent variables may cause a misclassification. SVM is a"}, {"start": 115.75999999999999, "end": 120.63999999999999, "text": " specially effective in cases where number of dimensions are more than the number of samples."}, {"start": 120.63999999999999, "end": 126.16, "text": " When finding the decision boundary, SVM uses a subset of training points rather than all points,"}, {"start": 126.16, "end": 131.2, "text": " which makes it memory efficient. On the other hand, training time increases for large data sets,"}, {"start": 131.2, "end": 136.39999999999998, "text": " which negatively affects the performance. Naive Bayes is a supervised learning algorithm used"}, {"start": 136.39999999999998, "end": 142.23999999999998, "text": " for classification tasks. Hence, it is also called Naive Bayes classifier. Naive Bayes assumes that"}, {"start": 142.23999999999998, "end": 147.2, "text": " features are independent of each other and there is no correlation between features. However,"}, {"start": 147.2, "end": 151.83999999999997, "text": " this is not the case in real life. This naive assumption of features being uncorrelated is the"}, {"start": 151.83999999999997, "end": 157.12, "text": " reason why this algorithm is called naive. The intuition behind naive Bayes algorithm is the"}, {"start": 157.12, "end": 163.28, "text": " Bayes theorem. PAB is the probability of events A given events B has already occurred."}, {"start": 163.84, "end": 170.8, "text": " PBA is the probability of events B given events A has already occurred. PA is the probability of"}, {"start": 170.8, "end": 177.44, "text": " events A and PB is the probability of events B. Naive Bayes classifier calculates the probability of"}, {"start": 177.44, "end": 182.24, "text": " a class given a set of feature values. The assumption that all features are independent makes"}, {"start": 182.24, "end": 187.68, "text": " Naive Bayes algorithm very fast when compared to complicated algorithms. In some cases, speed is"}, {"start": 187.68, "end": 193.04000000000002, "text": " preferred over higher accuracy. But on the other hand, the same assumption makes Naive Bayes algorithm"}, {"start": 193.04000000000002, "end": 198.16, "text": " less accurate than complicated algorithms. Logistic regression is a supervised learning algorithm"}, {"start": 198.16, "end": 203.84, "text": " which is mostly used for binary classification problems. Logistic regression is a simple yet very"}, {"start": 203.84, "end": 208.64000000000001, "text": " effective classification algorithm, so it is commonly used for many binary classification tasks."}, {"start": 208.72, "end": 214.16, "text": " Things like customer churns, spam, email, website, or ad click predictions are some examples of"}, {"start": 214.16, "end": 218.72, "text": " the areas where logistic regression offers a powerful solution. The basis of logistic regression"}, {"start": 218.72, "end": 224.72, "text": " is the logistic function, also called the sigmoid function, which takes any real value number and"}, {"start": 224.72, "end": 230.64, "text": " maps it to a value between zero and one. Let's consider we have the following linear equation to solve."}, {"start": 230.64, "end": 235.6, "text": " Logistic regression model takes a linear equation as input and uses logistic function and log"}, {"start": 236.07999999999998, "end": 242.48, "text": " to perform a binary classification task. Then we will get the famous shaped graph of logistic regression."}, {"start": 242.48, "end": 247.68, "text": " We can use the calculated probability as is. For example, the output can be the probability that"}, {"start": 247.68, "end": 254.95999999999998, "text": " this email is spam is 95%, or the probability that the customer will click on the ad is 70%. However,"}, {"start": 254.95999999999998, "end": 260.08, "text": " in most cases, probabilities are used to classify data points. For example, if the probability is"}, {"start": 260.08, "end": 267.28, "text": " greater than 50%, the prediction is positive class, or one. Otherwise, the prediction is negative class"}, {"start": 267.28, "end": 273.2, "text": " or zero. K nearest neighbors, or K and N for short, is a supervised learning algorithm that can be"}, {"start": 273.2, "end": 278.88, "text": " used to solve both classification and regression tasks. The main idea behind K and N is that the"}, {"start": 278.88, "end": 284.08, "text": " value of a class or of a data point is determined by the data points around it. K and N class"}, {"start": 284.08, "end": 290.08, "text": " fire determines the class of a data point by majority voting principle. For instance, if K is set"}, {"start": 290.08, "end": 295.2, "text": " to five, the classes of five closest points are checked. Prediction is done according to the"}, {"start": 295.2, "end": 300.88, "text": " majority class. Similarly, K and N regression takes the mean value of five closest points. Let's go"}, {"start": 300.88, "end": 305.44, "text": " over an example. Consider the following data points that belong to four different classes."}, {"start": 305.44, "end": 310.71999999999997, "text": " And let's see how the predicted classes change according to the K value. It is very important to"}, {"start": 310.72, "end": 316.40000000000003, "text": " determine an optimal K value. If K is too low, the model is too specific and not generalized well."}, {"start": 316.40000000000003, "end": 321.12, "text": " It also tends to be too sensitive to noise. The model accomplishes a high accuracy on"}, {"start": 321.12, "end": 326.48, "text": " train set, but will be a poor predictor on new previously unseen data points. Therefore,"}, {"start": 326.48, "end": 331.44000000000005, "text": " we are likely to end up with an overfit model. On the other hand, if K is too large,"}, {"start": 331.44000000000005, "end": 336.24, "text": " the model is too generalized and does not have a predictor on both train and test sets."}, {"start": 336.24, "end": 341.12, "text": " The situation is known as underfitting. K and N is simple and easy to interpret. It does not"}, {"start": 341.12, "end": 346.48, "text": " make any assumptions, so it can be implemented in non-linear tasks. K and N does become very slow"}, {"start": 346.48, "end": 350.40000000000003, "text": " as number of data points increases because the model needs to store all data points. Thus,"}, {"start": 350.40000000000003, "end": 355.2, "text": " it is not memory efficient. Another downside of K and N is that it is sensitive to outliers."}, {"start": 355.2, "end": 361.2, "text": " Decision trees work by iteratively asking questions to partition data. It is easier to conceptualize"}, {"start": 361.2, "end": 366.08, "text": " the partitioning data with a visual representation of the decision tree. This represents a decision"}, {"start": 366.08, "end": 371.44, "text": " tree to predict customer churn. First, split is based on monthly chart's amount. Then the algorithm"}, {"start": 371.44, "end": 376.64, "text": " keeps asking questions to separate class labels. The question gets more specific as the tree gets"}, {"start": 376.64, "end": 381.76, "text": " deeper. The aim is to increase the predictivis as much as possible at each partitioning so that"}, {"start": 381.76, "end": 385.91999999999996, "text": " the model keeps gaining information about the data set. Randomly splitting, the feature does not"}, {"start": 385.91999999999996, "end": 390.79999999999995, "text": " usually give us the valuable insight into the data set. It lets that increase purity of nodes"}, {"start": 390.8, "end": 396.08, "text": " that are most informative. The purity of a node is inversely proportional to the distribution"}, {"start": 396.08, "end": 401.44, "text": " of different classes in that node. The questions to ask are chosen in a way that increases purity"}, {"start": 401.44, "end": 406.8, "text": " or decreases impurity. But how many questions do we ask when do we stop? When is our tree sufficient"}, {"start": 406.8, "end": 411.28000000000003, "text": " to solve our classification problem? The answer to all of these questions leads us to one of the most"}, {"start": 411.28000000000003, "end": 416.64, "text": " important concepts in machine learning overfitting. The model can keep asking questions until all nodes"}, {"start": 417.12, "end": 421.28, "text": " are pure. However, this would be a two specific model and it would not generalize will. It achieves"}, {"start": 421.28, "end": 427.2, "text": " high accuracy with training set but performs poorly on new previously unseen data points which indicates"}, {"start": 427.2, "end": 432.24, "text": " overfitting. The decision tree algorithm usually does not require to normalize or scale features."}, {"start": 432.24, "end": 436.8, "text": " It is also suitable to work on a mixture of feature data types. On the negative side, it is"}, {"start": 436.8, "end": 442.64, "text": " prone to overfitting and needs to be unsombled in order to generalize will. Random forest is an"}, {"start": 442.64, "end": 447.68, "text": " ensemble of many decision trees. Random forests are built using a method called bagging in which"}, {"start": 447.68, "end": 453.12, "text": " decision trees are used as parallel estimators. If you use for a classification problem, the result is"}, {"start": 453.12, "end": 457.91999999999996, "text": " based on majority vote of the results received from each decision tree. For regression, the prediction"}, {"start": 457.91999999999996, "end": 463.28, "text": " of a leaf node is the mean value of the target values in that leaf. Random forest regression takes"}, {"start": 463.28, "end": 468.96, "text": " mean values of results from decision trees. Random forests reduce the risk of overfitting and accuracy"}, {"start": 469.03999999999996, "end": 474.23999999999995, "text": " is much higher than a single decision tree. Furthermore, decision trees in a random forest run in"}, {"start": 474.23999999999995, "end": 479.68, "text": " parallel so that the time does not become a bottleneck. The success of a random forest highly depends"}, {"start": 479.68, "end": 485.59999999999997, "text": " on using uncorrelated decision trees. If we use the same or very similar trees, the overall result"}, {"start": 485.59999999999997, "end": 490.15999999999997, "text": " will not be much different than the result of a single decision tree. Random forests achieve to"}, {"start": 490.15999999999997, "end": 496.08, "text": " have uncorrelated decision trees by bootstrapping and feature randomness. Bootstrapping is randomly"}, {"start": 496.08, "end": 500.96, "text": " selecting samples from training data with replacement. They are called the bootstrapped samples."}, {"start": 500.96, "end": 506.0, "text": " Feature randomness is achieved by selecting features randomly for each decision tree in a random"}, {"start": 506.0, "end": 511.03999999999996, "text": " forest. The number of features used for each tree in a random forest can be controlled with max"}, {"start": 511.03999999999996, "end": 516.16, "text": " underscore features parameter. Random forest is a highly accurate model on many different problems"}, {"start": 516.16, "end": 521.04, "text": " and does not require normalization or scaling. However, it is not a good choice for high-dimensional"}, {"start": 521.12, "end": 528.24, "text": " datasets compared to fast linear models. Gradient boosted decision trees or GBDT for short is an"}, {"start": 528.24, "end": 533.8399999999999, "text": " ensemble algorithm which uses boosting methods to combine individual decision trees. Boosting means"}, {"start": 533.8399999999999, "end": 538.88, "text": " combining a learning algorithm in series to achieve a strong learner from many sequentially"}, {"start": 538.88, "end": 545.12, "text": " connected weak learners. In the case of GBDT, the weak learners are the decision trees. Each tree"}, {"start": 545.12, "end": 550.5600000000001, "text": " attempts to minimize the errors of previous tree. Trees in boosting are weak learners but adding"}, {"start": 550.5600000000001, "end": 555.84, "text": " many trees in series and each focusing on the errors from the previous one make boosting a highly"}, {"start": 555.84, "end": 560.8, "text": " efficient and accurate model. Unlike bagging, boosted does not involve bootstrapped sampling."}, {"start": 560.8, "end": 565.76, "text": " Every time a new tree is added, it fits on a modified version of the initial dataset."}, {"start": 565.76, "end": 571.12, "text": " Since trees are added sequentially, boosting algorithms learn slowly. In statistical learning,"}, {"start": 571.12, "end": 577.52, "text": " models that learn slowly perform better. GBDT is very efficient on both classification and regression"}, {"start": 577.52, "end": 582.48, "text": " tasks and provides more accurate predictions compared to random forests. It can handle mixed"}, {"start": 582.48, "end": 588.0, "text": " type of features and no preprocessing is needed. GBDT does require careful tuning of hyper parameters"}, {"start": 588.0, "end": 592.88, "text": " in order to prevent the model from overfitting. K means clustering. Clustering is a way to group"}, {"start": 592.88, "end": 597.6, "text": " a set of data points in a way that similar data points are grouped together. Therefore, clustering"}, {"start": 597.6800000000001, "end": 603.36, "text": " algorithms look for similarities or dissimilarities among data points. Clustering is an unsupervised learning"}, {"start": 603.36, "end": 608.16, "text": " method so there is no label associated with data points. Clustering algorithms try to find the"}, {"start": 608.16, "end": 614.0, "text": " underlying structure of the data. Observations or data points in a classification task have labelled."}, {"start": 614.0, "end": 618.96, "text": " Each observation is classified according to some measurements. Classification algorithms try to"}, {"start": 618.96, "end": 623.84, "text": " model the relationship between measurements on observations and their assigned class. Then the"}, {"start": 623.9200000000001, "end": 629.44, "text": " model predicts the class of new observations. K means clustering aims to partition data into K"}, {"start": 629.44, "end": 634.08, "text": " clusters in a way that data points in the same cluster are similar and data points in different"}, {"start": 634.08, "end": 639.52, "text": " clusters are further apart. Thus, it is a partition-based clustering technique. Similarity of two"}, {"start": 639.52, "end": 644.48, "text": " points is determined by the distance between them. Consider the following to the visualization of a"}, {"start": 644.48, "end": 649.6, "text": " dataset. It can be parsioned into four different clusters. Now, real-life datasets are much more"}, {"start": 649.6, "end": 654.96, "text": " complex in which clusters are not clearly separated. However, the algorithm works in the same way."}, {"start": 654.96, "end": 660.32, "text": " K means is an iterative process. It is built on expectation, maximization, algorithm. After the"}, {"start": 660.32, "end": 665.44, "text": " number of clusters are determined, it works by executing the following steps. Number one, it randomly"}, {"start": 665.44, "end": 670.72, "text": " selects the centroid or the center of cluster for each cluster. Then it calculates the distance of"}, {"start": 670.72, "end": 676.1600000000001, "text": " all data points to the centroid. It assigns the data points to the closest cluster. It finds the new"}, {"start": 676.16, "end": 681.1999999999999, "text": " centroid of each cluster by taking the mean of all data points in the cluster. It repeats steps"}, {"start": 681.1999999999999, "end": 686.64, "text": " to three and four until all points converge and cluster centers stop moving. K means clustering"}, {"start": 686.64, "end": 691.1999999999999, "text": " is relatively fast and easy to interpret. It is also able to choose the positions of initial"}, {"start": 691.1999999999999, "end": 695.68, "text": " centroid in a smart way that speeds up the convergence. The one challenge with K means is that the"}, {"start": 695.68, "end": 701.1999999999999, "text": " number of clusters must be predetermined. K means algorithm is not able to guess how many clusters"}, {"start": 701.1999999999999, "end": 705.8399999999999, "text": " exist in the data. If there is a non-linear structure separating groups in the data,"}, {"start": 705.84, "end": 711.44, "text": " K means will not be a good choice. DB Stan clustering Partition based and hierarchical"}, {"start": 711.44, "end": 716.08, "text": " clustering techniques are highly efficient with normal shaped clusters. However, when it comes to"}, {"start": 716.08, "end": 722.1600000000001, "text": " arbitrary shaped clusters or detecting outliers, density based techniques are more efficient. DB"}, {"start": 722.1600000000001, "end": 728.1600000000001, "text": " Stan stands for density based spatial clustering of applications with noise. It is able to find arbitrary"}, {"start": 728.1600000000001, "end": 733.84, "text": " shaped clusters and clusters with noise. The main idea behind DB Stan is that a point belongs to a"}, {"start": 733.84, "end": 740.0, "text": " cluster if it is close to many points from that cluster. There are two key parameters of DB Stan."}, {"start": 740.0, "end": 744.5600000000001, "text": " EPS, which is the distance that specifies the neighborhood. Two points are considered to be"}, {"start": 744.5600000000001, "end": 750.64, "text": " neighbors if the distance between them are less than or equal to EPS and menPTS, which is the minimum"}, {"start": 750.64, "end": 755.44, "text": " number of data points to define a cluster. Based on these two parameters, points are classified as"}, {"start": 755.44, "end": 761.44, "text": " core point, border point, or outlet. A point is a core point if there are at least menPTS,"}, {"start": 761.44, "end": 767.84, "text": " number of points, including the point itself, in its surrounding area with radius EPS. A point"}, {"start": 767.84, "end": 772.8800000000001, "text": " is a border point if it is unreachable from a core point and there are less than menPTS number"}, {"start": 772.8800000000001, "end": 778.08, "text": " of points within its surrounding area. And a point is an outlier if it is not a core point"}, {"start": 778.08, "end": 783.6800000000001, "text": " and not reachable from any core points. DB Stan does not require to specify a number of clusters"}, {"start": 783.6800000000001, "end": 788.5600000000001, "text": " beforehand. It is robust to outliers and able to detect the outliers. In some cases,"}, {"start": 788.56, "end": 794.56, "text": " determining an appropriate distance of neighborhood EPS is not easy and it requires domain knowledge."}, {"start": 794.56, "end": 799.92, "text": " Principal components analysis or PCA is a dimensionally reduction algorithm which basically"}, {"start": 799.92, "end": 805.8399999999999, "text": " derives new features from the existing ones with keeping as much information as possible. PCA"}, {"start": 805.8399999999999, "end": 811.28, "text": " is an unsupervised learning algorithm, but it is also widely used as a pre-processing step for"}, {"start": 811.28, "end": 816.7199999999999, "text": " supervised learning algorithms. PCA derives new features by finding the relations among features"}, {"start": 816.72, "end": 821.84, "text": " in a dataset. The aim of PCA is to explain the variance within the original dataset as much"}, {"start": 821.84, "end": 827.6800000000001, "text": " as possible by using less features. The new derived features are called principal components."}, {"start": 827.6800000000001, "end": 831.44, "text": " The order of principal components is determined according to the fraction of variants"}, {"start": 831.44, "end": 836.8000000000001, "text": " of original dataset they explain. The advantage of PCA is that a significant amount of variance"}, {"start": 836.8000000000001, "end": 841.76, "text": " of the original dataset is retained using much smaller number of features than the original"}, {"start": 841.76, "end": 846.1600000000001, "text": " dataset. Principal components are ordered according to the amount of variants that they explain,"}, {"start": 846.16, "end": 849.92, "text": " and that is every common machine learning algorithm explained."}]}